{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Model\n",
    "from torch import nn\n",
    "\n",
    "class DirectionEmbeddingNet(nn.Module):\n",
    "    def __init__(self, in_channels, direction_dim, embedding_dim, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels (e.g., number of semantic classes).\n",
    "            direction_dim: Number of direction classes (e.g., 36).\n",
    "            embedding_dim: Dimension of the embedding output.\n",
    "            hidden_dim: Number of hidden channels in intermediate layers.\n",
    "        \"\"\"\n",
    "        super(DirectionEmbeddingNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.direction_head = nn.Conv2d(hidden_dim, direction_dim, kernel_size=1)\n",
    "        self.embedding_head = nn.Conv2d(hidden_dim, embedding_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input semantic map of shape (B, in_channels, H, W).\n",
    "        Returns:\n",
    "            direction_map: Direction map of shape (B, direction_dim, H, W).\n",
    "            embedding_map: Embedding map of shape (B, embedding_dim, H, W).\n",
    "        \"\"\"\n",
    "        features = self.encoder(x)\n",
    "        direction_map = self.direction_head(features)\n",
    "        embedding_map = self.embedding_head(features)\n",
    "        return direction_map, embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from data.dataset import semantic_dataset  # Assuming this provides the dataset\n",
    "from evaluation.angle_diff import calc_angle_diff  # For evaluation\n",
    "import logging\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from loss import DiscriminativeLoss\n",
    "\n",
    "# Define the training function\n",
    "def train_direction_embedding_model(args):\n",
    "    if not os.path.exists(args['logdir']):\n",
    "        os.makedirs(args['logdir'])\n",
    "    logging.basicConfig(filename=os.path.join(args['logdir'], \"results.log\"),\n",
    "                        filemode='w',\n",
    "                        format='%(asctime)s: %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "    logging.getLogger('shapely.geos').setLevel(logging.CRITICAL)\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "    # Create the dataset and dataloaders\n",
    "    data_conf = {\n",
    "        'num_channels': args['num_classes'] + 1,\n",
    "        'image_size': args['image_size'],\n",
    "        'xbound': args['xbound'],\n",
    "        'ybound': args['ybound'],\n",
    "        'zbound': args['zbound'],\n",
    "        'dbound': args['dbound'],\n",
    "        'thickness': args['thickness'],\n",
    "        'angle_class': args['angle_class'],\n",
    "    }\n",
    "    train_loader, val_loader = semantic_dataset(args['version'], args['dataroot'], data_conf, args['bsz'], args['nworkers'])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DirectionEmbeddingNet(\n",
    "        in_channels=args['num_classes'] + 1,\n",
    "        direction_dim=args['angle_class'] + 1,\n",
    "        embedding_dim=args['embedding_dim']\n",
    "    ).cuda()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = nn.DataParallel(model).to(device)\n",
    "\n",
    "    # Define the loss functions and optimizer\n",
    "    direction_criterion = nn.BCELoss(reduction='none')\n",
    "    embedding_criterion = DiscriminativeLoss(args['embedding_dim'], args['delta_v'], args['delta_d']).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    scheduler = StepLR(optimizer, 10, 0.1)\n",
    "\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    writer = SummaryWriter(log_dir=args['logdir'])    \n",
    "\n",
    "    # Store best model in a variable\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args['nepochs']):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch [{epoch+1}/{args['nepochs']}]\")\n",
    "        for _, (_, _, _, _, _, _, _, _, _, _, semantic_gt, embedding_gt, direction_gt) in progress_bar:\n",
    "            t0 = time()\n",
    "            # Move data to GPU\n",
    "            semantic_gt = semantic_gt.cuda().float()  # Input\n",
    "            direction_gt = direction_gt.cuda()  # Direction labels\n",
    "            embedding_gt = embedding_gt.cuda()  # Embedding labels\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            direction_pred, embedding_pred = model(semantic_gt)\n",
    "\n",
    "            # Compute losses\n",
    "            # direction_loss = direction_criterion(direction_pred, direction_gt.argmax(dim=1))  # CrossEntropy expects class indices\n",
    "            lane_mask = (1 - direction_gt[:, 0]).unsqueeze(1)\n",
    "            direction_loss = direction_criterion(torch.softmax(direction_pred, 1), direction_gt)\n",
    "            direction_loss = (direction_loss * lane_mask).sum() / (lane_mask.sum() * direction_loss.shape[1] + 1e-6)\n",
    "            angle_diff = calc_angle_diff(direction_pred, direction_gt, args['angle_class'])\n",
    "            \n",
    "            var_loss, dist_loss, reg_loss = embedding_criterion(embedding_pred, embedding_gt)\n",
    "            # loss = direction_loss + embedding_loss  # Combine losses\n",
    "            loss = var_loss * args['scale_var'] + dist_loss * args['scale_dist'] + direction_loss * args['scale_direction']\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            counter += 1\n",
    "            t1 = time()\n",
    "            if counter % 10 == 0:\n",
    "                write_to_tensorboard('train', counter, writer, t0, direction_loss, angle_diff, var_loss, dist_loss, reg_loss, loss, t1)\n",
    "\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'direction_loss': direction_loss.item(), 'final_loss': loss.item()})\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_direction_loss = 0\n",
    "            total_angle_diff = 0\n",
    "            val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
    "            for _, (_, _, _, _, _, _, _, _, _, _, semantic_gt, embedding_gt, direction_gt) in val_progress_bar:\n",
    "                t0 = time()\n",
    "                semantic_gt = semantic_gt.cuda().float()\n",
    "                direction_gt = direction_gt.cuda()\n",
    "                embedding_gt = embedding_gt.cuda()\n",
    "\n",
    "                direction_pred, embedding_pred = model(semantic_gt)\n",
    "\n",
    "                lane_mask = (1 - direction_gt[:, 0]).unsqueeze(1)\n",
    "                direction_loss = direction_criterion(torch.softmax(direction_pred, 1), direction_gt)\n",
    "                direction_loss = (direction_loss * lane_mask).sum() / (lane_mask.sum() * direction_loss.shape[1] + 1e-6)\n",
    "                angle_diff = calc_angle_diff(direction_pred, direction_gt, args['angle_class'])\n",
    "                \n",
    "                var_loss, dist_loss, reg_loss = embedding_criterion(embedding_pred, embedding_gt)\n",
    "                loss = var_loss * args['scale_var'] + dist_loss * args['scale_dist'] + direction_loss * args['scale_direction']\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_direction_loss += direction_loss.item()\n",
    "                total_angle_diff += angle_diff\n",
    "                val_progress_bar.set_postfix({'val_loss': loss.item(), 'direction_loss': direction_loss.item()})\n",
    "                \n",
    "                counter2 += 1\n",
    "                t1 = time()\n",
    "                if counter % 10 == 0:\n",
    "                    write_to_tensorboard('val', counter, writer, t0, direction_loss, angle_diff, var_loss, dist_loss, reg_loss, loss, t1)\n",
    "            avg_loss = total_loss / len(val_loader)\n",
    "            if avg_loss < best_val_loss:\n",
    "                best_model = model\n",
    "                print(f\"Best model saved with validation loss: {avg_loss:.4f}\")\n",
    "                best_val_loss = avg_loss\n",
    "            avg_direction_loss = total_direction_loss / len(val_loader)\n",
    "            # avg_embedding_loss = total_embedding_loss / len(val_loader)\n",
    "            avg_angle_diff = total_angle_diff / len(val_loader)\n",
    "            print(f\"Validation Loss: {avg_loss:.4f}, Direction Loss: {avg_direction_loss:.4f}, Average Angle Difference: {avg_angle_diff:.4f}\")\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        torch.save(model.state_dict(), os.path.join(args['logdir'], f\"direction_embedding_model_epoch_{epoch+1}.pth\"))\n",
    "        print(f\"Model saved for epoch {epoch+1}\")\n",
    "        scheduler.step()\n",
    "    return best_model\n",
    "\n",
    "def write_to_tensorboard(title, counter, writer, t0, direction_loss, angle_diff, var_loss, dist_loss, reg_loss, loss, t1):\n",
    "    writer.add_scalar(f'{title}/step_time', t1 - t0, counter)\n",
    "    writer.add_scalar(f'{title}/var_loss', var_loss, counter)\n",
    "    writer.add_scalar(f'{title}/dist_loss', dist_loss, counter)\n",
    "    writer.add_scalar(f'{title}/reg_loss', reg_loss, counter)\n",
    "    writer.add_scalar(f'{title}/direction_loss', direction_loss, counter)\n",
    "    writer.add_scalar(f'{title}/final_loss', loss, counter)\n",
    "    writer.add_scalar(f'{title}/angle_diff', angle_diff, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logdir': './results/direction_embedding_net', 'dataroot': '/home/yoganandam/data/nuScenes/v1.0-mini/', 'version': 'v1.0-mini', 'nepochs': 30, 'bsz': 4, 'nworkers': 10, 'lr': 0.001, 'weight_decay': 1e-07, 'num_classes': 3, 'angle_class': 36, 'embedding_dim': 16, 'image_size': [128, 352], 'xbound': [-30.0, 30.0, 0.15], 'ybound': [-15.0, 15.0, 0.15], 'zbound': [-10.0, 10.0, 20.0], 'dbound': [4.0, 45.0, 1.0], 'thickness': 5, 'delta_v': 0.5, 'delta_d': 3.0, 'scale_var': 1.0, 'scale_dist': 1.0, 'scale_direction': 1.0, 'max_grad_norm': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: 100%|██████████| 80/80 [00:30<00:00,  2.63it/s, loss=1.5, direction_loss=0.103, final_loss=1.5]   \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.67it/s, val_loss=2.01, direction_loss=0.071]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with validation loss: 2.2910\n",
      "Validation Loss: 2.2910, Direction Loss: 0.1048, Average Angle Difference: 26.6373\n",
      "Model saved for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/30]: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s, loss=1.19, direction_loss=0.107, final_loss=1.19] \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.54it/s, val_loss=1.98, direction_loss=0.0676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with validation loss: 2.2236\n",
      "Validation Loss: 2.2236, Direction Loss: 0.1038, Average Angle Difference: 26.5776\n",
      "Model saved for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/30]: 100%|██████████| 80/80 [00:32<00:00,  2.44it/s, loss=1.43, direction_loss=0.116, final_loss=1.43] \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.59it/s, val_loss=1.99, direction_loss=0.0651] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with validation loss: 2.1988\n",
      "Validation Loss: 2.1988, Direction Loss: 0.1026, Average Angle Difference: 25.8094\n",
      "Model saved for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/30]: 100%|██████████| 80/80 [00:32<00:00,  2.48it/s, loss=1.28, direction_loss=0.109, final_loss=1.28]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.49it/s, val_loss=1.98, direction_loss=0.0655] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2010, Direction Loss: 0.1027, Average Angle Difference: 25.7997\n",
      "Model saved for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/30]: 100%|██████████| 80/80 [00:31<00:00,  2.55it/s, loss=1.25, direction_loss=0.109, final_loss=1.25]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.76it/s, val_loss=1.97, direction_loss=0.0651] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1993, Direction Loss: 0.1014, Average Angle Difference: 26.0690\n",
      "Model saved for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/30]: 100%|██████████| 80/80 [00:31<00:00,  2.58it/s, loss=1.33, direction_loss=0.0899, final_loss=1.33]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.67it/s, val_loss=1.97, direction_loss=0.0651] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2021, Direction Loss: 0.1020, Average Angle Difference: 25.8674\n",
      "Model saved for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/30]: 100%|██████████| 80/80 [00:32<00:00,  2.49it/s, loss=1.29, direction_loss=0.0813, final_loss=1.29]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.77it/s, val_loss=1.94, direction_loss=0.0633] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with validation loss: 2.1974\n",
      "Validation Loss: 2.1974, Direction Loss: 0.1007, Average Angle Difference: 26.0527\n",
      "Model saved for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/30]: 100%|██████████| 80/80 [00:31<00:00,  2.57it/s, loss=1.2, direction_loss=0.0972, final_loss=1.2]   \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.50it/s, val_loss=1.93, direction_loss=0.0632] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2013, Direction Loss: 0.0996, Average Angle Difference: 25.6086\n",
      "Model saved for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/30]: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s, loss=1.59, direction_loss=0.082, final_loss=1.59]   \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.60it/s, val_loss=1.95, direction_loss=0.0636] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2083, Direction Loss: 0.0991, Average Angle Difference: 24.7037\n",
      "Model saved for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/30]: 100%|██████████| 80/80 [00:31<00:00,  2.56it/s, loss=1.26, direction_loss=0.0878, final_loss=1.26]\n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.75it/s, val_loss=1.93, direction_loss=0.062] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2018, Direction Loss: 0.0984, Average Angle Difference: 23.5275\n",
      "Model saved for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/30]: 100%|██████████| 80/80 [00:31<00:00,  2.58it/s, loss=1.41, direction_loss=0.102, final_loss=1.41]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.50it/s, val_loss=1.95, direction_loss=0.0638] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2012, Direction Loss: 0.0979, Average Angle Difference: 21.4955\n",
      "Model saved for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/30]: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s, loss=1.29, direction_loss=0.0865, final_loss=1.29]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.63it/s, val_loss=1.95, direction_loss=0.0635] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1992, Direction Loss: 0.0981, Average Angle Difference: 21.5120\n",
      "Model saved for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/30]: 100%|██████████| 80/80 [00:31<00:00,  2.57it/s, loss=1.14, direction_loss=0.101, final_loss=1.14]   \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.70it/s, val_loss=1.94, direction_loss=0.0634] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1977, Direction Loss: 0.0979, Average Angle Difference: 21.4813\n",
      "Model saved for epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/30]: 100%|██████████| 80/80 [00:31<00:00,  2.57it/s, loss=1.29, direction_loss=0.088, final_loss=1.29]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.73it/s, val_loss=1.94, direction_loss=0.0634] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1989, Direction Loss: 0.0980, Average Angle Difference: 21.5784\n",
      "Model saved for epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/30]: 100%|██████████| 80/80 [00:31<00:00,  2.53it/s, loss=1.16, direction_loss=0.08, final_loss=1.16]    \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.72it/s, val_loss=1.95, direction_loss=0.064]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2002, Direction Loss: 0.0976, Average Angle Difference: 21.3058\n",
      "Model saved for epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/30]: 100%|██████████| 80/80 [00:31<00:00,  2.55it/s, loss=1.5, direction_loss=0.0789, final_loss=1.5]    \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.62it/s, val_loss=1.94, direction_loss=0.0636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1994, Direction Loss: 0.0978, Average Angle Difference: 21.3736\n",
      "Model saved for epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17/30]: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s, loss=1.08, direction_loss=0.0986, final_loss=1.08]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.57it/s, val_loss=1.95, direction_loss=0.0637] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1988, Direction Loss: 0.0976, Average Angle Difference: 21.4484\n",
      "Model saved for epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [18/30]: 100%|██████████| 80/80 [00:30<00:00,  2.58it/s, loss=1.5, direction_loss=0.0918, final_loss=1.5]    \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.58it/s, val_loss=1.94, direction_loss=0.0635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1990, Direction Loss: 0.0977, Average Angle Difference: 21.3316\n",
      "Model saved for epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [19/30]: 100%|██████████| 80/80 [00:31<00:00,  2.56it/s, loss=1.49, direction_loss=0.078, final_loss=1.49]   \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.63it/s, val_loss=1.94, direction_loss=0.0631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1995, Direction Loss: 0.0978, Average Angle Difference: 21.3397\n",
      "Model saved for epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20/30]: 100%|██████████| 80/80 [00:31<00:00,  2.58it/s, loss=1.5, direction_loss=0.0846, final_loss=1.5]    \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.53it/s, val_loss=1.94, direction_loss=0.0633] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2008, Direction Loss: 0.0975, Average Angle Difference: 21.3046\n",
      "Model saved for epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [21/30]: 100%|██████████| 80/80 [00:30<00:00,  2.64it/s, loss=1.63, direction_loss=0.0814, final_loss=1.63]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.68it/s, val_loss=1.94, direction_loss=0.0633] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1999, Direction Loss: 0.0975, Average Angle Difference: 21.2936\n",
      "Model saved for epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [22/30]: 100%|██████████| 80/80 [00:31<00:00,  2.52it/s, loss=1.32, direction_loss=0.0982, final_loss=1.32]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.52it/s, val_loss=1.94, direction_loss=0.0633] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1994, Direction Loss: 0.0975, Average Angle Difference: 21.2851\n",
      "Model saved for epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [23/30]: 100%|██████████| 80/80 [00:31<00:00,  2.51it/s, loss=1.3, direction_loss=0.0793, final_loss=1.3]    \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.65it/s, val_loss=1.94, direction_loss=0.0633] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1995, Direction Loss: 0.0975, Average Angle Difference: 21.2714\n",
      "Model saved for epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [24/30]: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s, loss=1.64, direction_loss=0.0804, final_loss=1.64]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.65it/s, val_loss=1.94, direction_loss=0.0633] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1993, Direction Loss: 0.0975, Average Angle Difference: 21.2817\n",
      "Model saved for epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [25/30]: 100%|██████████| 80/80 [00:30<00:00,  2.63it/s, loss=1.12, direction_loss=0.0885, final_loss=1.12]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.62it/s, val_loss=1.94, direction_loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1993, Direction Loss: 0.0974, Average Angle Difference: 21.2696\n",
      "Model saved for epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [26/30]: 100%|██████████| 80/80 [00:32<00:00,  2.50it/s, loss=1.36, direction_loss=0.0724, final_loss=1.36] \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.69it/s, val_loss=1.94, direction_loss=0.0633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1991, Direction Loss: 0.0974, Average Angle Difference: 21.2664\n",
      "Model saved for epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [27/30]: 100%|██████████| 80/80 [00:31<00:00,  2.57it/s, loss=1.56, direction_loss=0.0911, final_loss=1.56]  \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.64it/s, val_loss=1.94, direction_loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1991, Direction Loss: 0.0974, Average Angle Difference: 21.2684\n",
      "Model saved for epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [28/30]: 100%|██████████| 80/80 [00:31<00:00,  2.55it/s, loss=1.28, direction_loss=0.078, final_loss=1.28]   \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.58it/s, val_loss=1.94, direction_loss=0.0633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1990, Direction Loss: 0.0974, Average Angle Difference: 21.2494\n",
      "Model saved for epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [29/30]: 100%|██████████| 80/80 [00:32<00:00,  2.48it/s, loss=1.21, direction_loss=0.119, final_loss=1.21]   \n",
      "Validation: 100%|██████████| 21/21 [00:07<00:00,  2.72it/s, val_loss=1.94, direction_loss=0.0633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1991, Direction Loss: 0.0974, Average Angle Difference: 21.2689\n",
      "Model saved for epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [30/30]: 100%|██████████| 80/80 [00:32<00:00,  2.45it/s, loss=1.33, direction_loss=0.0851, final_loss=1.33]  \n",
      "Validation: 100%|██████████| 21/21 [00:08<00:00,  2.61it/s, val_loss=1.94, direction_loss=0.0634] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1990, Direction Loss: 0.0974, Average Angle Difference: 21.2194\n",
      "Model saved for epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        \"logdir\": \"./results/direction_embedding_net\",\n",
    "        \"dataroot\": \"/home/yoganandam/data/nuScenes/v1.0-mini/\",\n",
    "        \"version\": \"v1.0-mini\",  # Choices: [\"v1.0-trainval\", \"v1.0-mini\"]\n",
    "        \"nepochs\": 30,\n",
    "        \"bsz\": 4,\n",
    "        \"nworkers\": 10,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-7,\n",
    "        \"num_classes\": 3,  # Number of semantic classes\n",
    "        \"angle_class\": 36,\n",
    "        \"embedding_dim\": 16,\n",
    "        \"image_size\": [128, 352],\n",
    "        \"xbound\": [-30.0, 30.0, 0.15],\n",
    "        \"ybound\": [-15.0, 15.0, 0.15],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [4.0, 45.0, 1.0],\n",
    "        \"thickness\": 5,\n",
    "        \"delta_v\": 0.5,\n",
    "        \"delta_d\": 3.0,\n",
    "        \"scale_var\": 1.0,\n",
    "        \"scale_dist\": 1.0,\n",
    "        \"scale_direction\": 1.0,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "    }\n",
    "    print(args)\n",
    "    best_model = train_direction_embedding_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoganandam/miniconda3/envs/hdmapnet/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/home/yoganandam/miniconda3/envs/hdmapnet/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/yoganandam/miniconda3/envs/hdmapnet/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Export results\n",
    "import argparse\n",
    "import mmcv\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "from data.dataset import semantic_dataset\n",
    "from data.const import NUM_CLASSES\n",
    "from model import get_model\n",
    "from postprocess.vectorize import vectorize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_dx_bx(xbound, ybound):\n",
    "    dx = [row[2] for row in [xbound, ybound]]\n",
    "    bx = [row[0] + row[2] / 2.0 for row in [xbound, ybound]]\n",
    "    nx = [(row[1] - row[0]) / row[2] for row in [xbound, ybound]]\n",
    "    return dx, bx, nx\n",
    "\n",
    "def export_to_json(model, val_loader, angle_class, args):\n",
    "    submission = {\n",
    "        \"meta\": {\n",
    "            \"use_camera\": True,\n",
    "            \"use_lidar\": False,\n",
    "            \"use_radar\": False,\n",
    "            \"use_external\": False,\n",
    "            \"vector\": True,\n",
    "        },\n",
    "        \"results\": {}\n",
    "    }\n",
    "\n",
    "    dx, bx, nx = gen_dx_bx(args['xbound'], args['ybound'])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
    "        for batchi, (_, _, _, _, _, _, _, _, _, _, segmentation, embedding_gt, direction_gt) in val_progress_bar:\n",
    "            segmentation = segmentation.cuda().float()\n",
    "            direction, embedding = model(segmentation)\n",
    "            for si in range(segmentation.shape[0]):\n",
    "                coords, confidences, line_types = vectorize(segmentation[si], embedding[si], direction[si], angle_class)\n",
    "                # vectors = []\n",
    "                # for coord, confidence, line_type in zip(coords, confidences, line_types):\n",
    "                #     vector = {'pts': coord * dx + bx, 'pts_num': len(coord), \"type\": line_type, \"confidence_level\": confidence}\n",
    "                #     vectors.append(vector)\n",
    "                # rec = val_loader.dataset.samples[batchi * val_loader.batch_size + si]\n",
    "                # submission['results'][rec['token']] = vectors\n",
    "                for coord in coords:\n",
    "                    plt.plot(coord[:, 0], coord[:, 1], linewidth=5)\n",
    "\n",
    "                plt.xlim((0, segmentation.shape[3]))\n",
    "                plt.ylim((0, segmentation.shape[2]))\n",
    "                # plt.imshow(car_img, extent=[segmentation.shape[3]//2-15, segmentation.shape[3]//2+15, segmentation.shape[2]//2-12, segmentation.shape[2]//2+12])\n",
    "\n",
    "                img_name = f'/home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval{batchi:06}_{si:03}.jpg'\n",
    "                # print('saving', img_name)\n",
    "                plt.savefig(img_name)\n",
    "                plt.close()\n",
    "\n",
    "    mmcv.dump(submission, args['output'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logdir': './results/direction_embedding_net', 'dataroot': '/home/yoganandam/data/nuScenes/v1.0-mini/', 'version': 'v1.0-mini', 'nepochs': 30, 'bsz': 4, 'nworkers': 10, 'lr': 0.001, 'weight_decay': 1e-07, 'num_classes': 3, 'angle_class': 36, 'embedding_dim': 16, 'image_size': [128, 352], 'xbound': [-30.0, 30.0, 0.15], 'ybound': [-15.0, 15.0, 0.15], 'zbound': [-10.0, 10.0, 20.0], 'dbound': [4.0, 45.0, 1.0], 'thickness': 5, 'delta_v': 0.5, 'delta_d': 3.0, 'scale_var': 1.0, 'scale_dist': 1.0, 'scale_direction': 1.0, 'max_grad_norm': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        \"logdir\": \"./results/direction_embedding_net\",\n",
    "        \"dataroot\": \"/home/yoganandam/data/nuScenes/v1.0-mini/\",\n",
    "        \"version\": \"v1.0-mini\",  # Choices: [\"v1.0-trainval\", \"v1.0-mini\"]\n",
    "        \"nepochs\": 30,\n",
    "        \"bsz\": 4,\n",
    "        \"nworkers\": 10,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-7,\n",
    "        \"num_classes\": 3,  # Number of semantic classes\n",
    "        \"angle_class\": 36,\n",
    "        \"embedding_dim\": 16,\n",
    "        \"image_size\": [128, 352],\n",
    "        \"xbound\": [-30.0, 30.0, 0.15],\n",
    "        \"ybound\": [-15.0, 15.0, 0.15],\n",
    "        \"zbound\": [-10.0, 10.0, 20.0],\n",
    "        \"dbound\": [4.0, 45.0, 1.0],\n",
    "        \"thickness\": 5,\n",
    "        \"delta_v\": 0.5,\n",
    "        \"delta_d\": 3.0,\n",
    "        \"scale_var\": 1.0,\n",
    "        \"scale_dist\": 1.0,\n",
    "        \"scale_direction\": 1.0,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "    }\n",
    "    print(args)\n",
    "    data_conf = {\n",
    "        'num_channels': args['num_classes'] + 1,\n",
    "        'image_size': args['image_size'],\n",
    "        'xbound': args['xbound'],\n",
    "        'ybound': args['ybound'],\n",
    "        'zbound': args['zbound'],\n",
    "        'dbound': args['dbound'],\n",
    "        'thickness': args['thickness'],\n",
    "        'angle_class': args['angle_class'],\n",
    "    }\n",
    "    train_loader, val_loader = semantic_dataset(args['version'], args['dataroot'], data_conf, args['bsz'], args['nworkers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = DirectionEmbeddingNet(\n",
    "        in_channels=args['num_classes'] + 1,\n",
    "        direction_dim=args['angle_class'] + 1,\n",
    "        embedding_dim=args['embedding_dim']\n",
    "    ).cuda()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = nn.DataParallel(best_model).to(device)\n",
    "best_model.load_state_dict(torch.load('/home/yoganandam/workspace/Online-HDMap/HDMapNet/results/direction_embedding_net/direction_embedding_model_epoch_30.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000000_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000000_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000000_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   5%|▍         | 1/21 [00:15<05:13, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000000_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000001_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000001_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000001_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  10%|▉         | 2/21 [00:24<03:45, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000001_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000002_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000002_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000002_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  14%|█▍        | 3/21 [00:35<03:22, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000002_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000003_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000003_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000003_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  19%|█▉        | 4/21 [00:41<02:33,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000003_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000004_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000004_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000004_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  24%|██▍       | 5/21 [00:46<02:04,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000004_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000005_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000005_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000005_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  29%|██▊       | 6/21 [00:51<01:42,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000005_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000006_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000006_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000006_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 7/21 [00:57<01:31,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000006_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000007_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000007_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000007_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  38%|███▊      | 8/21 [01:02<01:17,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000007_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000008_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000008_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000008_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  43%|████▎     | 9/21 [01:10<01:19,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000008_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000009_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000009_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000009_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  48%|████▊     | 10/21 [01:23<01:36,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000009_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000010_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000010_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000010_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  52%|█████▏    | 11/21 [01:28<01:15,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000010_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000011_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000011_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000011_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  57%|█████▋    | 12/21 [01:32<00:58,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000011_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000012_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000012_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000012_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  62%|██████▏   | 13/21 [01:37<00:46,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000012_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000013_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000013_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000013_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  67%|██████▋   | 14/21 [01:42<00:39,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000013_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000014_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000014_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000014_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  71%|███████▏  | 15/21 [01:47<00:33,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000014_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000015_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000015_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000015_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  76%|███████▌  | 16/21 [01:52<00:26,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000015_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000016_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000016_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000016_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  81%|████████  | 17/21 [01:57<00:20,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000016_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000017_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000017_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000017_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  86%|████████▌ | 18/21 [02:01<00:15,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000017_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000018_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000018_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000018_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  90%|█████████ | 19/21 [02:07<00:10,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000018_003.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000019_000.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000019_001.jpg\n",
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000019_002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  95%|█████████▌| 20/21 [02:14<00:05,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000019_003.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [02:17<00:00,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/yoganandam/workspace/Online-HDMap/HDMapNet/results/imgs/eval000020_000.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Export results\n",
    "import argparse\n",
    "import mmcv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from data.dataset import semantic_dataset\n",
    "from data.const import NUM_CLASSES\n",
    "from model import get_model\n",
    "from postprocess.vectorize import vectorize\n",
    "args = {\n",
    "    \"dataroot\": \"dataset/nuScenes/\",\n",
    "    \"version\": \"v1.0-mini\",\n",
    "    \"model\": \"HDMapNet_cam\",\n",
    "    \"bsz\": 4,\n",
    "    \"nworkers\": 10,\n",
    "    \"modelf\": None,\n",
    "    \"thickness\": 5,\n",
    "    \"image_size\": [128, 352],\n",
    "    \"xbound\": [-30.0, 30.0, 0.15],\n",
    "    \"ybound\": [-15.0, 15.0, 0.15],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [4.0, 45.0, 1.0],\n",
    "    \"embedding_dim\": 16,\n",
    "    \"angle_class\": 36,\n",
    "    \"output\": \"output.json\"\n",
    "}\n",
    "data_conf = {\n",
    "    'num_channels': NUM_CLASSES + 1,\n",
    "    'image_size': args['image_size'],\n",
    "    'xbound': args['xbound'],\n",
    "    'ybound': args['ybound'],\n",
    "    'zbound': args['zbound'],\n",
    "    'dbound': args['dbound'],\n",
    "    'thickness': args['thickness'],\n",
    "    'angle_class': args['angle_class'],\n",
    "}\n",
    "export_to_json(best_model, val_loader, args['angle_class'], args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdmapnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
